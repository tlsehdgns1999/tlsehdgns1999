{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "1HvKi2kSuOhogEVbvIR37K4qBOfrVwDpf",
      "authorship_tag": "ABX9TyPJ4VQVNLhPeRMa01O+jeRo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlsehdgns1999/tlsehdgns1999/blob/main/samsung_deep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiyDR_yQDZnE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !unzip --q '/content/drive/MyDrive/open-4/open.zip (1).zip의 사본'"
      ],
      "metadata": {
        "id": "CghhoAOw4srM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1\n",
        "NUM_CLASSES = 11\n",
        "DATA_DIR = \"/content/test_image\"\n",
        "DATA_DIR_MSK = \"/content/train_source_gt\"\n",
        "VAL_DATA_DIR = \"/content/val_source_image\"\n",
        "VAL_DATA_DIR_MSK = \"/content/val_source_gt\"\n",
        "NUM_TRAIN_IMAGES = 500\n",
        "NUM_VAL_IMAGES = 50\n",
        "IMAGE_SIZE = 1024\n",
        "\n",
        "image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']\n",
        "\n",
        "train_images = []\n",
        "for ext in image_extensions:\n",
        "    train_images.extend(glob(os.path.join(DATA_DIR, f'*{ext}')))\n",
        "\n",
        "train_masks = []\n",
        "for ext in image_extensions:\n",
        "    train_masks.extend(glob(os.path.join(DATA_DIR, f'*{ext}')))\n",
        "\n",
        "val_images = []\n",
        "for ext in image_extensions:\n",
        "    val_images.extend(glob(os.path.join(DATA_DIR, f'*{ext}')))\n",
        "\n",
        "val_masks = []\n",
        "for ext in image_extensions:\n",
        "    val_masks.extend(glob(os.path.join(DATA_DIR, f'*{ext}')))"
      ],
      "metadata": {
        "id": "9kqIDMfdv4BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###   1\n",
        "\n",
        "'''\n",
        "train_images = sorted(glob(os.path.join(DATA_DIR)))[:NUM_TRAIN_IMAGES]\n",
        "train_masks = sorted(glob(os.path.join(DATA_DIR_MSK)))[:NUM_TRAIN_IMAGES]\n",
        "val_images = sorted(glob(os.path.join(VAL_DATA_DIR)))[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "\n",
        "val_masks = sorted(glob(os.path.join(VAL_DATA_DIR_MSK)))[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "'''\n",
        "\n",
        "###   2\n",
        "'''\n",
        "train_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[:NUM_TRAIN_IMAGES]\n",
        "train_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[:NUM_TRAIN_IMAGES]\n",
        "val_images = sorted(glob(os.path.join(DATA_DIR, \"Images/*\")))[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "\n",
        "val_masks = sorted(glob(os.path.join(DATA_DIR, \"Category_ids/*\")))[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "'''"
      ],
      "metadata": {
        "id": "PyVbqt3YzRSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(image_path, mask=False):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    if mask:\n",
        "        image = tf.image.decode_png(image, channels=1)\n",
        "        image.set_shape([None, None, 1])\n",
        "        image = tf.image.resize(images=image, size=[2048, 1024])##\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, channels=3)\n",
        "        image.set_shape([None, None, 3])\n",
        "        image = tf.image.resize(images=image, size=[2048, 1024])###\n",
        "        image = tf.keras.applications.resnet50.preprocess_input(image)\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "J-KBWNulzWZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(image_list, mask_list):\n",
        "    image = read_image(image_list)\n",
        "    mask = read_image(mask_list, mask=True)\n",
        "    return image, mask"
      ],
      "metadata": {
        "id": "JClMciI9zYMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(image_list, mask_list):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "45s90eD5zZjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = data_generator(train_images, train_masks)\n",
        "val_dataset = data_generator(val_images, val_masks)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)"
      ],
      "metadata": {
        "id": "9SFg8ujbDlnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=128, ###\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output"
      ],
      "metadata": {
        "id": "XKQCaouT3jW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(2048, 1024, 3))\n",
        "    resnet50 = keras.applications.ResNet50( ###\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(2048 // 4 // x.shape[1], 1024 // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(2048 // x.shape[1], 1024 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "\n",
        "model = DeeplabV3Plus(image_size=(2048, 1024), num_classes=NUM_CLASSES)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "u-je1B9L3kDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001), ##\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=25)\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.title(\"Training Loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"Validation Loss\")\n",
        "plt.ylabel(\"val_loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.ylabel(\"val_accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_gqIJASB3nUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "duMWFTay0Pj6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}